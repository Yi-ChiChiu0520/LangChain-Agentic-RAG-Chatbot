import json
import os
import re
from typing import List
import smtplib
from email.message import EmailMessage

import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

from dotenv import load_dotenv
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain.schema import Document
from langchain_core.tools import tool
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.agents import AgentExecutor, create_react_agent
from langchain_core.runnables import RunnableWithMessageHistory
from langchain_community.chat_message_histories import FileChatMessageHistory

# ===== åŸºæœ¬è¨­å®š =====
DB_DIR = "chroma_db"  # å‘é‡è³‡æ–™åº«å„²å­˜ç›®éŒ„
COLLECTION = "my-collection"  # Chroma è³‡æ–™åº«é›†åˆåç¨±
SESSION_ID = "ethan-test-new"  # Changed session ID to start fresh
load_dotenv()  # è¼‰å…¥ .env.exampleï¼ˆSENDER_EMAIL / SENDER_PASSWORDï¼‰


# --- å·¥å…·å‡½å¼ ---
def flatten_text(text: str) -> str:
    """å£“ç¸®å¤šé¤˜ç©ºç™½ï¼Œé¿å… reranker å› æ›è¡Œ/é›œè¨Šå½±éŸ¿åˆ†æ•¸"""
    return re.sub(r"\s+", " ", text).strip()


# ===== è¼‰å…¥äº¤å‰ç·¨ç¢¼å™¨ Rerankerï¼ˆå•Ÿå‹•æ™‚åªè¼‰ä¸€æ¬¡ï¼‰=====
_RERANKER_TOK = None
_RERANKER = None

try:
    _RERANKER_TOK = AutoTokenizer.from_pretrained("BAAI/bge-reranker-base")
    _RERANKER = AutoModelForSequenceClassification.from_pretrained("BAAI/bge-reranker-base")

    # Check if CUDA is available and move model to appropriate device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    _RERANKER = _RERANKER.to(device)
    _RERANKER.eval()
    print(f"âœ… Reranker loaded successfully on {device}")
except Exception as e:
    print(f"âš ï¸ Failed to load reranker model: {e}")
    print("âš ï¸ Reranking will be disabled, using original retrieval order")


def rerank(query: str, docs: List[Document], top_k: int = 5) -> List[Document]:
    """
    ä½¿ç”¨äº¤å‰ç·¨ç¢¼å™¨å°æª¢ç´¢åˆ°çš„æ®µè½é‡æ–°æ’åºï¼Œå›å‚³å‰ top_k æ®µã€‚
    å¦‚æœæ¨¡å‹è¼‰å…¥å¤±æ•—ï¼Œå‰‡å›å‚³åŸå§‹é †åºçš„å‰ top_k æ®µã€‚
    """
    if _RERANKER is None or _RERANKER_TOK is None:
        print("[RERANK] Model not available, using original order")
        return docs[:top_k]

    try:
        scored = []
        device = next(_RERANKER.parameters()).device

        for d in docs:
            text = flatten_text(d.page_content)
            inputs = _RERANKER_TOK(query, text, return_tensors="pt", truncation=True)
            inputs = {k: v.to(device) for k, v in inputs.items()}

            with torch.no_grad():
                score = _RERANKER(**inputs).logits.item()
            scored.append((score, d))
            # é™¤éŒ¯ï¼šæª¢è¦–æ¯æ®µåˆ†æ•¸
            print(f"[RERANK] score={score:.4f} | meta={d.metadata}")

        scored.sort(key=lambda x: x[0], reverse=True)
        return [d for _, d in scored[:top_k]]
    except Exception as e:
        print(f"[RERANK] Error during reranking: {e}, using original order")
        return docs[:top_k]


# ===== RAG å…ƒä»¶ï¼šEmbeddingã€VectorStoreã€Retriever =====
try:
    embedding = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")
    print("âœ… Embedding model loaded successfully")
except Exception as e:
    print(f"âš ï¸ Failed to load embedding model: {e}")
    print("âš ï¸ Using fallback embedding model")
    try:
        embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        print("âœ… Fallback embedding model loaded successfully")
    except Exception as e2:
        print(f"âŒ Failed to load fallback embedding model: {e2}")
        raise RuntimeError("No embedding model available")

try:
    vectorstore = Chroma(
        collection_name=COLLECTION,
        embedding_function=embedding,
        persist_directory=DB_DIR,
    )
    print("âœ… Vector store initialized successfully")
except Exception as e:
    print(f"âš ï¸ Failed to initialize vector store: {e}")
    raise RuntimeError("Vector store initialization failed")

# åˆå§‹å¬å› k ç­†ï¼ˆäº¤çµ¦ reranker å†æ’åºï¼‰
retriever = vectorstore.as_retriever(search_kwargs={"k": 8})

# ===== LLMï¼ˆOpenAIï¼‰=====
try:
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.0)
    print("âœ… LLM initialized successfully")
except Exception as e:
    print(f"âš ï¸ Failed to initialize LLM: {e}")
    print("âš ï¸ Please ensure OPENAI_API_KEY is set in your environment")
    raise RuntimeError("LLM initialization failed")


# ===== å·¥å…·ï¼ˆToolsï¼‰=====
@tool("fetch_rag_context", return_direct=True)
def fetch_rag_context(question: str) -> str:
    """
    æ ¹æ“šå•é¡Œï¼Œå¾å‘é‡åº«æŠ“å–ç›¸é—œæ®µè½ä¸¦é‡æ’åºï¼Œå›å‚³å®Œæ•´çš„ç­”æ¡ˆã€‚
    """
    print(f"Searching for: {question}")
    initial_docs = retriever.invoke(question)

    if not initial_docs:
        return "æ ¹æ“šæª¢ç´¢çµæœï¼ŒæŸ¥ç„¡ç›¸é—œè³‡æ–™ã€‚"

    top_docs = rerank(question, initial_docs, top_k=5)
    context = "\n\n---\n\n".join([d.page_content for d in top_docs])
    print(f"Context found: {context[:200]}...")

    # Use LLM to generate answer based on context
    qa_prompt = ChatPromptTemplate.from_messages([
        ("system",
         "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„å•ç­”åŠ©æ‰‹ã€‚æ ¹æ“šæä¾›çš„èƒŒæ™¯è³‡æ–™å›ç­”ç”¨æˆ¶å•é¡Œã€‚\n\n"
         "è¦å‰‡ï¼š\n"
         "1. åªæ ¹æ“šèƒŒæ™¯è³‡æ–™å›ç­”ï¼Œä¸è¦æ·»åŠ èƒŒæ™¯è³‡æ–™ä¸­æ²’æœ‰çš„è³‡è¨Š\n"
         "2. å¦‚æœèƒŒæ™¯è³‡æ–™ä¸­æ²’æœ‰ç­”æ¡ˆï¼Œè«‹æ˜ç¢ºèªªæ˜\n"
         "3. å›ç­”è¦ç°¡æ½”æ˜ç¢º\n"
         "4. ä¿æŒå®¢è§€ä¸­æ€§\n\n"
         "èƒŒæ™¯è³‡æ–™ï¼š\n{context}"),
        ("human", "å•é¡Œï¼š{question}")
    ])

    qa_chain = qa_prompt | llm
    response = qa_chain.invoke({
        "context": context,
        "question": question
    })
    return response.content.strip()


@tool("send_email", return_direct=True)
def send_email_tool(input_data: str) -> str:
    """
    å¯„é€ Emailã€‚è¼¸å…¥æ‡‰ç‚º JSON å­—ä¸²æ ¼å¼ï¼š
    {"to": "æ”¶ä»¶äººemail", "subject": "ä¸»æ—¨", "body": "å…§å®¹"}
    ä½¿ç”¨ Gmailï¼ˆéœ€åœ¨ .env.example è¨­å®š SENDER_EMAIL / SENDER_PASSWORD App Passwordï¼‰
    """
    sender = os.getenv("SENDER_EMAIL")
    pwd = (os.getenv("SENDER_PASSWORD") or "").replace(" ", "")
    if not sender or not pwd:
        return "âš ï¸ ç¼ºå°‘ SENDER_EMAIL / SENDER_PASSWORD ç’°å¢ƒè®Šæ•¸ã€‚"

    try:
        # Parse the JSON input
        if isinstance(input_data, str):
            data = json.loads(input_data)
        else:
            data = input_data

        to = data["to"]
        subject = data["subject"]
        body = data["body"]

        msg = EmailMessage()
        msg["From"] = sender
        msg["To"] = to
        msg["Subject"] = subject
        msg.set_content(body)

        with smtplib.SMTP("smtp.gmail.com", 587) as s:
            s.starttls()
            s.login(sender, pwd)
            s.send_message(msg)

        print(f"[EMAIL]\nTO: {to}\nSUBJECT: {subject}\nBODY:\n{body}\n")
        return f"âœ… å·²å¯„å‡ºè‡³ {to} [[EMAIL_SENT]]"  # Added marker for system prompt
    except json.JSONDecodeError as e:
        return f"âš ï¸ JSON è§£æéŒ¯èª¤ï¼š{e}"
    except KeyError as e:
        return f"âš ï¸ ç¼ºå°‘å¿…è¦æ¬„ä½ï¼š{e}"
    except Exception as e:
        return f"âš ï¸ å¯„ä¿¡å¤±æ•—ï¼š{e}"


@tool("answer_and_email", return_direct=True)
def answer_and_email_tool(input_data: str) -> str:
    """
    å›ç­”å•é¡Œä¸¦åŒæ™‚å¯„é€ç­”æ¡ˆåˆ°æŒ‡å®šçš„ emailã€‚è¼¸å…¥æ‡‰ç‚º JSON å­—ä¸²æ ¼å¼ï¼š
    {"question": "è¦å›ç­”çš„å•é¡Œ", "email": "æ”¶ä»¶äººemail", "subject": "éƒµä»¶ä¸»æ—¨ï¼ˆå¯é¸ï¼‰"}
    """
    try:
        if isinstance(input_data, str):
            data = json.loads(input_data)
        else:
            data = input_data

        question = data["question"]
        email = data["email"]
        subject = data.get("subject", "å›ç­”æ‚¨çš„å•é¡Œ")

        # First, get the answer using RAG
        print(f"Searching for: {question}")
        initial_docs = retriever.invoke(question)

        if not initial_docs:
            answer = "æ ¹æ“šæª¢ç´¢çµæœï¼ŒæŸ¥ç„¡ç›¸é—œè³‡æ–™ã€‚"
        else:
            top_docs = rerank(question, initial_docs, top_k=5)
            context = "\n\n---\n\n".join([d.page_content for d in top_docs])
            print(f"Context found: {context[:200]}...")

            # Use LLM to generate answer based on context
            qa_prompt = ChatPromptTemplate.from_messages([
                ("system",
                 "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„å•ç­”åŠ©æ‰‹ã€‚æ ¹æ“šæä¾›çš„èƒŒæ™¯è³‡æ–™å›ç­”ç”¨æˆ¶å•é¡Œã€‚\n\n"
                 "è¦å‰‡ï¼š\n"
                 "1. åªæ ¹æ“šèƒŒæ™¯è³‡æ–™å›ç­”ï¼Œä¸è¦æ·»åŠ èƒŒæ™¯è³‡æ–™ä¸­æ²’æœ‰çš„è³‡è¨Š\n"
                 "2. å¦‚æœèƒŒæ™¯è³‡æ–™ä¸­æ²’æœ‰ç­”æ¡ˆï¼Œè«‹æ˜ç¢ºèªªæ˜\n"
                 "3. å›ç­”è¦ç°¡æ½”æ˜ç¢º\n"
                 "4. ä¿æŒå®¢è§€ä¸­æ€§\n\n"
                 "èƒŒæ™¯è³‡æ–™ï¼š\n{context}"),
                ("human", "å•é¡Œï¼š{question}")
            ])

            qa_chain = qa_prompt | llm
            response = qa_chain.invoke({
                "context": context,
                "question": question
            })
            answer = response.content.strip()

        # Send email
        sender = os.getenv("SENDER_EMAIL")
        pwd = (os.getenv("SENDER_PASSWORD") or "").replace(" ", "")
        if not sender or not pwd:
            return f"ç­”æ¡ˆï¼š{answer}\nâš ï¸ ç¼ºå°‘ SENDER_EMAIL / SENDER_PASSWORD ç’°å¢ƒè®Šæ•¸ï¼Œç„¡æ³•å¯„ä¿¡ã€‚"

        msg = EmailMessage()
        msg["From"] = sender
        msg["To"] = email
        msg["Subject"] = subject
        msg.set_content(answer)

        with smtplib.SMTP("smtp.gmail.com", 587) as s:
            s.starttls()
            s.login(sender, pwd)
            s.send_message(msg)

        print(f"[EMAIL]\nTO: {email}\nSUBJECT: {subject}\nBODY:\n{answer}\n")
        return f"ç­”æ¡ˆï¼š{answer}\nâœ… å·²å¯„å‡ºè‡³ {email}"

    except json.JSONDecodeError as e:
        return f"âš ï¸ JSON è§£æéŒ¯èª¤ï¼š{e}"
    except KeyError as e:
        return f"âš ï¸ ç¼ºå°‘å¿…è¦æ¬„ä½ï¼š{e}"
    except Exception as e:
        return f"âš ï¸ è™•ç†è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}"


tools = [fetch_rag_context, send_email_tool, answer_and_email_tool]

# ===== ReAct Agentï¼ˆç”±æç¤ºæ±ºå®šæ˜¯å¦ä½¿ç”¨å·¥å…·ï¼‰=====
react_prompt = ChatPromptTemplate.from_messages([
    ("system",
     "ä½ æ˜¯åŠ©ç† Adamã€‚ä½ å¿…é ˆåš´æ ¼æŒ‰ç…§ ReAct æ ¼å¼å›æ‡‰ã€‚\n\n"
     "é‡è¦ï¼šä»”ç´°åˆ†æç”¨æˆ¶çš„è«‹æ±‚ï¼\n\n"
     "å·¥å…·ä½¿ç”¨è¦å‰‡ï¼š\n"
     "- å¦‚æœç”¨æˆ¶å•å•é¡Œä¸¦æ˜ç¢ºè¦æ±‚å¯„ä¿¡ï¼ˆåŒ…å«ã€Œå¯„çµ¦ã€ã€ã€Œå‚³çµ¦ã€ã€ã€Œå¯„ä¿¡çµ¦ã€ã€ã€Œç™¼é€çµ¦ã€ã€ã€Œemail toã€ç­‰é—œéµå­—ï¼‰ï¼Œä½¿ç”¨ answer_and_email å·¥å…·\n"
     "- å¦‚æœç”¨æˆ¶åªæ˜¯å•å•é¡Œï¼ˆæ²’æœ‰å¯„ä¿¡è¦æ±‚ï¼‰ï¼Œä½¿ç”¨ fetch_rag_context å·¥å…·\n"
     "- å¦‚æœç”¨æˆ¶åªæ˜¯è¦æ±‚å¯„ä¿¡å·²çŸ¥å…§å®¹ï¼ˆä¸éœ€è¦æŸ¥è©¢ï¼‰ï¼Œä½¿ç”¨ send_email å·¥å…·\n\n"
     "æ³¨æ„ï¼š\n"
     "1. å¦‚æœå•é¡Œéœ€è¦å¾è³‡æ–™åº«æŸ¥è©¢ç­”æ¡ˆä¸¦å¯„ä¿¡ï¼Œä½¿ç”¨ answer_and_email\n"
     "2. å¦‚æœå•é¡Œéœ€è¦å¾è³‡æ–™åº«æŸ¥è©¢ç­”æ¡ˆä½†ä¸å¯„ä¿¡ï¼Œä½¿ç”¨ fetch_rag_context\n"
     "3. å¦‚æœåªæ˜¯å¯„ä¿¡å·²çŸ¥å…§å®¹ï¼Œä½¿ç”¨ send_email\n\n"
     "ä½ å¿…é ˆåš´æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›æ‡‰ï¼š\n"
     "Thought: [ä»”ç´°åˆ†æç”¨æˆ¶è«‹æ±‚ï¼š1)å•äº†ä»€éº¼å•é¡Œï¼Ÿ2)æ˜¯å¦è¦æ±‚å¯„ä¿¡ï¼Ÿ3)æ˜¯å¦éœ€è¦æŸ¥è©¢è³‡æ–™ï¼Ÿ4)æ‡‰è©²ä½¿ç”¨å“ªå€‹å·¥å…·ï¼Ÿ]\n"
     "Action: [å·¥å…·åç¨±]\n"
     "Action Input: [è¼¸å…¥å…§å®¹]\n\n"
     "å¯ç”¨å·¥å…·ï¼š{tools}\n"
     "å·¥å…·åç¨±ï¼š{tool_names}\n\n"
     "ç¯„ä¾‹ 1ï¼ˆåªå•å•é¡Œï¼Œéœ€è¦æŸ¥è©¢ï¼‰ï¼š\n"
     "ç”¨æˆ¶ï¼šä»€éº¼æ˜¯ç¾è€Œç¾ï¼Ÿ\n"
     "ä½ çš„å›æ‡‰ï¼š\n"
     "Thought: ç”¨æˆ¶å•äº†é—œæ–¼ç¾è€Œç¾çš„å•é¡Œï¼Œéœ€è¦æŸ¥è©¢è³‡æ–™ï¼Œä½†æ²’æœ‰è¦æ±‚å¯„ä¿¡ï¼Œæˆ‘éœ€è¦ä½¿ç”¨ fetch_rag_context\n"
     "Action: fetch_rag_context\n"
     "Action Input: \"ä»€éº¼æ˜¯ç¾è€Œç¾ï¼Ÿ\"\n\n"
     "ç¯„ä¾‹ 2ï¼ˆå•å•é¡Œ+å¯„ä¿¡ï¼Œéœ€è¦æŸ¥è©¢ï¼‰ï¼š\n"
     "ç”¨æˆ¶ï¼šä»€éº¼æ˜¯ç¾è€Œç¾ï¼Ÿä¸¦å¯„çµ¦ test@example.com\n"
     "ä½ çš„å›æ‡‰ï¼š\n"
     "Thought: ç”¨æˆ¶å•äº†å•é¡Œä¸¦è¦æ±‚å¯„ä¿¡ï¼Œéœ€è¦æŸ¥è©¢è³‡æ–™ï¼Œæˆ‘éœ€è¦ä½¿ç”¨ answer_and_email å·¥å…·\n"
     "Action: answer_and_email\n"
     "Action Input: {{\"question\": \"ä»€éº¼æ˜¯ç¾è€Œç¾ï¼Ÿ\", \"email\": \"test@example.com\", \"subject\": \"å›ç­”æ‚¨çš„å•é¡Œ\"}}\n\n"
     "ç¯„ä¾‹ 3ï¼ˆåªå¯„ä¿¡å·²çŸ¥å…§å®¹ï¼‰ï¼š\n"
     "ç”¨æˆ¶ï¼šå¯„é€\"Hello World\"çµ¦ test@example.com\n"
     "ä½ çš„å›æ‡‰ï¼š\n"
     "Thought: ç”¨æˆ¶è¦æ±‚å¯„ä¿¡å·²çŸ¥å…§å®¹ï¼Œä¸éœ€è¦æŸ¥è©¢è³‡æ–™ï¼Œæˆ‘éœ€è¦ä½¿ç”¨ send_email å·¥å…·\n"
     "Action: send_email\n"
     "Action Input: {{\"to\": \"test@example.com\", \"subject\": \"ä¿¡ä»¶å…§å®¹\", \"body\": \"Hello World\"}}"
     ),
    MessagesPlaceholder("chat_history"),
    ("human", "{input}"),
    ("ai", "{agent_scratchpad}"),
])

agent = create_react_agent(llm=llm, tools=tools, prompt=react_prompt)
agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    verbose=True,
    handle_parsing_errors="éŒ¯èª¤ï¼šä½ åŒæ™‚è¼¸å‡ºäº† Action å’Œ Final Answerã€‚è«‹åªè¼¸å‡ºä¸€å€‹æ­¥é©Ÿï¼šè¦å˜›æ˜¯ 'Thought/Action/Action Input'ï¼Œè¦å˜›æ˜¯ 'Final Answer'ï¼Œä¸èƒ½åŒæ™‚è¼¸å‡ºã€‚è«‹é‡æ–°é–‹å§‹ç•¶å‰æ­¥é©Ÿã€‚",
    max_iterations=2,  # Reduced since all tools return directly
    early_stopping_method="force",  # Changed back to force since generate is not supported
    return_intermediate_steps=False
)


# ===== å°è©±è¨˜æ†¶ï¼ˆæŒä¹…åŒ–åˆ°æª”æ¡ˆï¼‰=====
def get_session_history(session_id: str):
    os.makedirs("chat_logs", exist_ok=True)
    return FileChatMessageHistory(f"chat_logs/{session_id}.json")


# æŠŠ AgentExecutor åŒ…è£é€²è¨˜æ†¶æ©Ÿåˆ¶
agent_with_memory = RunnableWithMessageHistory(
    agent_executor,
    get_session_history=get_session_history,
    input_messages_key="input",
    history_messages_key="chat_history",
)


# ===== å°å¤–å›ç­”ä»‹é¢ï¼ˆå–®ä¸€è·¯å¾‘ï¼šäº¤çµ¦ Agent æ±ºç­–æ˜¯å¦ç”¨å·¥å…·ï¼‰=====
def answer(user_input: str) -> str:
    """
    äº¤çµ¦ ReAct Agentï¼šå®ƒæœƒå…ˆ fetch_rag_contextï¼Œå†å›ç­”ï¼›
    è‹¥ä½¿ç”¨è€…è¦æ±‚å¯„ä¿¡ï¼Œå°±æœƒè‡ªå‹•å‘¼å« send_emailã€‚
    """
    try:
        result = agent_with_memory.invoke(
            {"input": user_input},
            config={"configurable": {"session_id": SESSION_ID}},
        )
        # AgentExecutor çš„è¼¸å‡ºæ˜¯ dictï¼Œæœ€çµ‚ç­”æ¡ˆåœ¨ "output"
        return result.get("output", str(result))
    except Exception as e:
        print(f"Error in answer function: {e}")
        return f"æŠ±æ­‰ï¼Œè™•ç†æ‚¨çš„è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}"


# ===== CLI =====
if __name__ == "__main__":
    print("ğŸ” å•Ÿå‹•èŠå¤©ï¼ˆAgent æ¨¡å¼ï¼‰ï¼Œè¼¸å…¥ 'exit' é›¢é–‹")
    while True:
        q = input("\nä½ çš„å•é¡Œï¼š").strip()
        if q.lower() in {"exit", "quit"}:
            break
        print("\n--- ç­”æ¡ˆ ---")
        print(answer(q))